{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576c0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen #Urllib package is the URL handling module for python. It is used to fetch URLs.\n",
    "import bs4                         #Beautiful Soup is a Python package for parsing HTML and XML documents\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d35bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewsArchive():\n",
    "    url = urlopen('https://www.spiegel.de/international/') #Crawl URL\n",
    "    data = soup(url.read())\n",
    "    index = len(data.find_all('div',{\"data-block-el\":\"articleTeaser\"})) #find how many articles\n",
    "    print(\"There are \" + str(index) + \" articles.\")\n",
    "    i = 0\n",
    "    while i < index:\n",
    "        data_area = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].get(\"data-area\")\n",
    "        if data_area == 'article_teaser>news-xxl':\n",
    "            title = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('span')[1].text.strip()\n",
    "            subtitle = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('span')[0].text.strip()\n",
    "            abstract = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('a')[2].text.strip()\n",
    "            print(\"Article: \" + str(i+1))\n",
    "            print(\"Title: \"+title)\n",
    "            print(\"Subtitle: \"+subtitle)\n",
    "            print(\"Abstract: \"+abstract)\n",
    "            print()\n",
    "            i+=1\n",
    "        else: #the article that is not complete or in other forms\n",
    "            check_a = len(data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('a'))\n",
    "            title = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('span')[check_a-1].text.strip()\n",
    "            subtitle = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('span')[check_a-2].text.strip()\n",
    "            abstract = data.find_all('div',{\"data-block-el\":\"articleTeaser\"})[i].find_all('a')[check_a-1].text.strip()\n",
    "            print(\"Article: \" + str(i+1))\n",
    "            print(\"Title: \"+title)\n",
    "            print(\"Subtitle: \"+subtitle)\n",
    "            print(\"Abstract: \"+abstract)\n",
    "            print()\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "interval = 900 #every 60*15 mins = 900 second\n",
    "def automatic_run(interval): #run the interval\n",
    "    while True:\n",
    "        clear_output(wait=True) #clear output\n",
    "        NewsArchive() #run a desire function\n",
    "        \n",
    "        now = datetime.now() #print local time\n",
    "        download_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Download Time =\", download_time)\n",
    "        \n",
    "        time.sleep(interval) #wait to run again\n",
    "        \n",
    "        \n",
    "automatic_run(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f988e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
